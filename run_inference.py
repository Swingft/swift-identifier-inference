#!/usr/bin/env python3
"""
run_inference.py

RunPod GPU í™˜ê²½ì—ì„œ Swift ì‹ë³„ì ì¶”ì¶œ ì‹¤í–‰
- JSONL ë°ì´í„°ì…‹ ì…ë ¥
- ì¤‘ë‹¨ ì‹œ ì¬ê°œ ê¸°ëŠ¥
- ì§„í–‰ìƒí™© ì €ì¥
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any, Set
from tqdm import tqdm
import argparse
from llama_cpp import Llama


class SwiftIdentifierExtractor:
    """Swift ì†ŒìŠ¤ì½”ë“œì—ì„œ ì‹ë³„ì ì¶”ì¶œ"""

    def __init__(self,
                 base_model_path: str,
                 lora_path: str,
                 n_ctx: int = 8192,
                 n_gpu_layers: int = -1,  # -1 = ëª¨ë“  ë ˆì´ì–´ GPUì— ë¡œë“œ
                 checkpoint_file: str = "checkpoint/processed.txt"):
        """
        Args:
            base_model_path: ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ
            lora_path: LoRA ì–´ëŒ‘í„° ê²½ë¡œ
            n_ctx: ì»¨í…ìŠ¤íŠ¸ í¬ê¸°
            n_gpu_layers: GPU ë ˆì´ì–´ ìˆ˜ (-1 = ì „ì²´)
            checkpoint_file: ì²˜ë¦¬ ì™„ë£Œëœ íŒŒì¼ ê¸°ë¡
        """
        self.checkpoint_file = checkpoint_file
        self.processed_files = self._load_checkpoint()

        print("=" * 60)
        print("ëª¨ë¸ ë¡œë”© ì¤‘...")
        print("=" * 60)
        print(f"Base model: {base_model_path}")
        print(f"LoRA adapter: {lora_path}")
        print(f"Context size: {n_ctx}")
        print(f"GPU layers: {n_gpu_layers if n_gpu_layers != -1 else 'ALL'}")

        try:
            self.model = Llama(
                model_path=base_model_path,
                lora_path=lora_path,
                n_ctx=n_ctx,
                n_gpu_layers=n_gpu_layers,
                n_threads=4,
                verbose=False,
                use_mmap=True,
                use_mlock=False,
            )
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def _load_checkpoint(self) -> Set[str]:
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì—ì„œ ì²˜ë¦¬ ì™„ë£Œëœ íŒŒì¼ ëª©ë¡ ë¡œë“œ"""
        if not os.path.exists(self.checkpoint_file):
            return set()

        with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
            processed = set(line.strip() for line in f if line.strip())

        if processed:
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {len(processed)}ê°œ íŒŒì¼ ì´ë¯¸ ì²˜ë¦¬ë¨")

        return processed

    def _save_checkpoint(self, filename: str):
        """ì²˜ë¦¬ ì™„ë£Œëœ íŒŒì¼ì„ ì²´í¬í¬ì¸íŠ¸ì— ì €ì¥"""
        os.makedirs(os.path.dirname(self.checkpoint_file), exist_ok=True)
        with open(self.checkpoint_file, 'a', encoding='utf-8') as f:
            f.write(f"{filename}\n")
        self.processed_files.add(filename)

    def create_prompt(self, item: Dict[str, Any]) -> str:
        """ì¶”ë¡  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        code = item.get('code', '')
        repo = item.get('repo', 'unknown')
        filename = item.get('filename', 'unknown')

        # ëª…í™•í•œ í”„ë¡¬í”„íŠ¸
        prompt = f"""Analyze the following Swift code and extract all identifiers that should be excluded from obfuscation.

**Repository:** {repo}
**File:** {filename}

**Swift Code:**
```swift
{code}
```

Return ONLY a valid JSON object in this exact format:
{{"identifiers": ["identifier1", "identifier2", "identifier3"]}}

Do not include any explanations or additional text. Only return the JSON object."""

        return prompt

    def extract_identifiers(self, item: Dict[str, Any]) -> List[str]:
        """ë‹¨ì¼ í•­ëª©ì—ì„œ ì‹ë³„ì ì¶”ì¶œ"""
        filename = item.get('filename', 'unknown')

        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ìŠ¤í‚µ
        if filename in self.processed_files:
            return []

        prompt = self.create_prompt(item)

        try:
            response = self.model.create_chat_completion(
                messages=[
                    {"role": "system", "content": "You are a Swift code analyzer. Always return valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                top_p=0.95,
                max_tokens=2048,
                stop=["```", "\n\n\n"]  # ì¡°ê¸° ì¢…ë£Œë¡œ ê¹”ë”í•œ ì¶œë ¥
            )

            output = response['choices'][0]['message']['content'].strip()

            # JSON íŒŒì‹±
            identifiers = self._parse_output(output)

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            self._save_checkpoint(filename)

            return identifiers

        except Exception as e:
            print(f"\nâš ï¸  ì—ëŸ¬ ë°œìƒ ({filename}): {e}")
            return []

    def _parse_output(self, output: str) -> List[str]:
        """ëª¨ë¸ ì¶œë ¥ì—ì„œ ì‹ë³„ì ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (ê°•ë ¥í•œ íŒŒì‹±)"""
        import re

        # ë°©ë²• 1: ê¹¨ë—í•œ JSON ë¸”ë¡ íŒŒì‹±
        try:
            start_idx = output.find('{')
            end_idx = output.rfind('}')

            if start_idx != -1 and end_idx != -1:
                json_str = output[start_idx:end_idx + 1]

                # ì£¼ì„ ì œê±°
                json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)
                json_str = re.sub(r'//.*?$', '', json_str, flags=re.MULTILINE)

                data = json.loads(json_str)
                if 'identifiers' in data and isinstance(data['identifiers'], list):
                    identifiers = [str(id).strip() for id in data['identifiers'] if id]
                    return [id for id in identifiers if id and id != 'identifiers']
        except:
            pass

        # ë°©ë²• 2: ë°°ì—´ë§Œ ì¶”ì¶œ ["id1", "id2"]
        try:
            array_match = re.search(r'\[([^\]]+)\]', output)
            if array_match:
                array_str = '[' + array_match.group(1) + ']'
                identifiers = json.loads(array_str)
                return [str(id).strip() for id in identifiers if id and str(id) != 'identifiers']
        except:
            pass

        # ë°©ë²• 3: ë”°ì˜´í‘œë¡œ ê°ì‹¼ ë¬¸ìì—´ë“¤ ì¶”ì¶œ
        try:
            identifiers = re.findall(r'"([^"]+)"', output)
            # í‚¤ì›Œë“œ í•„í„°ë§
            filtered = [
                id for id in identifiers
                if id not in ['identifiers', 'reasoning', 'error', 'exclusions', 'evidence']
                   and len(id) > 1  # ë„ˆë¬´ ì§§ì€ ê²ƒ ì œì™¸
                   and not id.startswith('is_')  # í”Œë˜ê·¸ ì œì™¸
                   and not id.startswith('This ')  # ì„¤ëª…ë¬¸ ì œì™¸
            ]
            if filtered:
                return filtered[:50]  # ìµœëŒ€ 50ê°œ
        except:
            pass

        return []

    def process_dataset(self,
                        dataset_path: str,
                        output_file: str = "output/identifiers.txt",
                        batch_size: int = 1) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        print("=" * 60)
        print("ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹œì‘")
        print("=" * 60)

        # ë°ì´í„°ì…‹ ë¡œë“œ
        dataset = []
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    dataset.append(json.loads(line))

        total_items = len(dataset)
        already_processed = len([item for item in dataset if item.get('filename') in self.processed_files])

        print(f"ì „ì²´ í•­ëª©: {total_items}ê°œ")
        print(f"ì²˜ë¦¬ ì™„ë£Œ: {already_processed}ê°œ")
        print(f"ì²˜ë¦¬ í•„ìš”: {total_items - already_processed}ê°œ\n")

        if already_processed == total_items:
            print("âœ… ëª¨ë“  í•­ëª©ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            return self._load_results(output_file)

        # ì¶”ë¡  ì‹¤í–‰
        all_identifiers = []

        with tqdm(total=total_items, desc="Processing", initial=already_processed) as pbar:
            for item in dataset:
                identifiers = self.extract_identifiers(item)
                all_identifiers.extend(identifiers)
                pbar.update(1)

                # ì‹¤ì‹œê°„ ì €ì¥ (ì¤‘ë³µ ì œê±° í›„)
                if identifiers:
                    self._save_identifiers(all_identifiers, output_file)

        # ìµœì¢… ê²°ê³¼
        unique_identifiers = sorted(list(set(all_identifiers)))
        self._save_identifiers(unique_identifiers, output_file)

        print("\n" + "=" * 60)
        print("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ì´ ì‹ë³„ì: {len(all_identifiers)}ê°œ")
        print(f"ê³ ìœ  ì‹ë³„ì: {len(unique_identifiers)}ê°œ")
        print(f"ì¶œë ¥ íŒŒì¼: {output_file}")

        return {
            "total_items": total_items,
            "processed_items": total_items,
            "total_identifiers": len(all_identifiers),
            "unique_identifiers": len(unique_identifiers),
            "output_file": output_file
        }

    def _save_identifiers(self, identifiers: List[str], output_file: str):
        """ì‹ë³„ì ëª©ë¡ì„ íŒŒì¼ì— ì €ì¥"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        unique_sorted = sorted(list(set(identifiers)))

        with open(output_file, 'w', encoding='utf-8') as f:
            for identifier in unique_sorted:
                f.write(f"{identifier}\n")

    def _load_results(self, output_file: str) -> Dict[str, Any]:
        """ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ"""
        if not os.path.exists(output_file):
            return {}

        with open(output_file, 'r', encoding='utf-8') as f:
            identifiers = [line.strip() for line in f if line.strip()]

        return {
            "total_items": len(self.processed_files),
            "processed_items": len(self.processed_files),
            "total_identifiers": len(identifiers),
            "unique_identifiers": len(identifiers),
            "output_file": output_file
        }


def main():
    parser = argparse.ArgumentParser(description='Swift ì‹ë³„ì ì¶”ì¶œ (GPU)')

    parser.add_argument(
        '--dataset',
        type=str,
        default='dataset.jsonl',
        help='ì…ë ¥ JSONL íŒŒì¼ (ê¸°ë³¸ê°’: dataset.jsonl)'
    )
    parser.add_argument(
        '--base_model',
        type=str,
        default='models/base_model.gguf',
        help='ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ'
    )
    parser.add_argument(
        '--lora',
        type=str,
        default='models/lora.gguf',
        help='LoRA ì–´ëŒ‘í„° ê²½ë¡œ'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='output/identifiers.txt',
        help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--ctx',
        type=int,
        default=8192,
        help='ì»¨í…ìŠ¤íŠ¸ í¬ê¸° (ê¸°ë³¸ê°’: 8192)'
    )
    parser.add_argument(
        '--gpu_layers',
        type=int,
        default=-1,
        help='GPU ë ˆì´ì–´ ìˆ˜ (ê¸°ë³¸ê°’: -1 = ì „ì²´)'
    )
    parser.add_argument(
        '--reset',
        action='store_true',
        help='ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™”'
    )

    args = parser.parse_args()

    # ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™”
    if args.reset:
        checkpoint_file = "checkpoint/processed.txt"
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
            print("âœ… ì²´í¬í¬ì¸íŠ¸ ì´ˆê¸°í™”ë¨\n")

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.dataset):
        print(f"âŒ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.dataset}")
        return

    if not os.path.exists(args.base_model):
        print(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.base_model}")
        return

    if not os.path.exists(args.lora):
        print(f"âŒ LoRA ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.lora}")
        return

    try:
        # ì¶”ì¶œê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
        extractor = SwiftIdentifierExtractor(
            base_model_path=args.base_model,
            lora_path=args.lora,
            n_ctx=args.ctx,
            n_gpu_layers=args.gpu_layers
        )

        results = extractor.process_dataset(
            dataset_path=args.dataset,
            output_file=args.output
        )

        # ê²°ê³¼ ìš”ì•½ ì €ì¥
        summary_path = args.output.replace('.txt', '_summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"\nìš”ì•½ íŒŒì¼: {summary_path}")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤")
        print("ğŸ’¾ ì§„í–‰ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒì— ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()